from functools import partial
from typing import Callable, Tuple
import jax
import jax.numpy as jnp

import src.kernels as kernels

@jax.jit
def standard_normal_pdf_divided_by_cdf(x:jax.Array) -> jax.Array:
    """Returns element-wise phi(x) / Phi(x) in a numerically stable fashion
    """
    return jax.lax.select(x < -10, -x, jax.scipy.stats.norm.pdf(x)/jax.scipy.stats.norm.cdf(x))


@partial(jax.jit, static_argnames=['kernel'])
def covariance_matrix_from_kernel(kernel:Callable, kernel_args:dict, positions:jax.Array) -> jax.Array:
    """Creates a covariance matrix from a kernel and positions

    Args:
        kernel (Callable): the kernel function to call, e.g. kernels.gaussian_kernel
        kernel_args (dict): the additional arguments supplied to the kernel, e.g. length_scale or amplitude. See the signature of kernel (Callable).
        positions (jax.Array): the positions on which to cross-evaluate the kernel

    Returns:
        jax.Array: A covariance matrix generated by cross-evaluating the kernel on a sequence of positions
    """
    return kernels.evaluate_kernel(positions, positions, kernel, kernel_args)
    
@jax.jit
def condition_on_observations(mean_vector:jax.Array, covariance_matrix:jax.Array, observation_indices:jax.Array, observations:jax.Array, observation_noise_stds:jax.Array) -> Tuple[jax.Array, jax.Array]:
    """Condition the random variable X ~ N(mean_vector, covariance_matrix) on observations 
    (with i.i.d. noise N(0, observation_noise_stds[i]^2)) at the observation_indices
    Consumes O(m^2 * n + m * n^2 + n^3) compute and O(m^2+n^2) memory

    Args:
        mean_vector (jax.Array): (m,) the mean vector of the to-be-conditioned random variable
        covariance_matrix (jax.Array): (m, m) the covariance matrix of the to-be-conditioned random variable
        observation_indices (jax.Array): (n,) the indices of the Gaussian components/coordinates that are observed
        observations (jax.Array): (n,) the actual, observed values
        observation_noise_stds (jax.Array): (n,) the standard deviations of the i.i.d. noise associated with the observations

    Returns:
        Tuple[jax.Array, jax.Array]: (the new mean vector, the new covariance matrix)
    """
    assert len(jnp.shape(mean_vector)) == 1, "the mean_vector must be a 1d array"
    assert jnp.shape(covariance_matrix) == (jnp.shape(mean_vector)[0], jnp.shape(mean_vector)[0]), "the covariance matrix must be of shape (m, m)"
    assert len(jnp.shape(observation_indices)) == 1, "observation_indices must be of shape (n,)"
    assert jnp.shape(observations) == jnp.shape(observation_indices), "observations should be of shape (n,) where (n,) is the shape of observation_indices"
    assert jnp.shape(observation_noise_stds) == jnp.shape(observation_indices), "observation_noise_stds should be of shape (n,) where (n,) is the shape of observation_indices"

    mu_A = mean_vector
    mu_B = mean_vector[observation_indices]

    Sigma_AA = covariance_matrix
    Sigma_AB = covariance_matrix[:, observation_indices]
    Sigma_BA = jnp.matrix_transpose(Sigma_AB)
    Sigma_BB = Sigma_AB[observation_indices, :] + jnp.diag(observation_noise_stds**2)
    inv_Sigma_BB = jnp.linalg.inv(Sigma_BB)

    new_mean_vector = mu_A + Sigma_AB @ (inv_Sigma_BB @ (observations - mu_B))
    new_covariance_matrix = Sigma_AA - Sigma_AB @ (inv_Sigma_BB @ Sigma_BA)

    return (new_mean_vector, new_covariance_matrix)

@jax.jit
def condition_on_observations_diagonal(mean_vector:jax.Array, covariance_matrix:jax.Array, observation_indices:jax.Array, observations:jax.Array, observation_noise_stds:jax.Array) -> Tuple[jax.Array, jax.Array]:
    """Condition the random variable X ~ N(mean_vector, covariance_matrix) on observations 
    (with i.i.d. noise N(0, observation_noise_stds[i]^2)) at the observation_indices, but only return the diagonal entries of the new covariance matrix.
    Consumes O(m * n^2 + n^3) compute and O(m * n + n^2) memory instead of O(m^2 * n + m * n^2 + n^3) compute and O(m^2+n^2) memory necessary to compute the complete covariance matrix

    Args:
        mean_vector (jax.Array): (m,) the mean vector of the to-be-conditioned random variable
        covariance_matrix (jax.Array): (m, m) the covariance matrix of the to-be-conditioned random variable
        observation_indices (jax.Array): (n,) the indices of the Gaussian components/coordinates that are observed
        observations (jax.Array): (n,) the actual, observed values
        observation_noise_stds (jax.Array): the standard deviations of the i.i.d. noise associated with the observations

    Returns:
        Tuple[jax.Array, jax.Array]: (the new mean vector, diagonal entries of the new covariance matrix in vector format)
    """
    assert len(jnp.shape(mean_vector)) == 1, "the mean_vector must be a 1d array"
    assert jnp.shape(covariance_matrix) == (jnp.shape(mean_vector)[0], jnp.shape(mean_vector)[0]), "the covariance matrix must be of shape (m, m)"
    assert len(jnp.shape(observation_indices)) == 1, "observation_indices must be of shape (n,)"
    assert jnp.shape(observations) == jnp.shape(observation_indices), "observations should be of shape (n,) where (n,) is the shape of observation_indices"
    assert jnp.shape(observation_noise_stds) == jnp.shape(observation_indices), "observation_noise_stds should be of shape (n,) where (n,) is the shape of observation_indices"

    mu_A = mean_vector
    mu_B = mean_vector[observation_indices]

    diag_o_Sigma_AA = jnp.diag(covariance_matrix)
    Sigma_AB = covariance_matrix[:, observation_indices]
    Sigma_BA = jnp.matrix_transpose(Sigma_AB)
    Sigma_BB = Sigma_AB[observation_indices, :] + jnp.diag(observation_noise_stds**2)
    inv_Sigma_BB = jnp.linalg.inv(Sigma_BB)

    new_mean_vector = mu_A + Sigma_AB @ (inv_Sigma_BB @ (observations - mu_B))
    diag_o_new_covariance_matrix = diag_o_Sigma_AA - jnp.sum(Sigma_AB.transpose() * (inv_Sigma_BB @ Sigma_BA), axis=0)

    return (new_mean_vector, diag_o_new_covariance_matrix)

@partial(jax.jit, static_argnames=['kernel'])
def condition_on_observations_diagonal_w_kernel(mean:float, kernel:Callable, kernel_params:dict, positions:jax.Array, observation_indices:jax.Array, observations:jax.Array, observation_noise_stds:jax.Array) -> Tuple[jax.Array, jax.Array]:
    """Condition the random variable X ~ N(mean, kernel@positions) on observations 
    (with i.i.d. noise N(0, observation_noise_stds[i]^2)) at the observation_indices, but only return the diagonal entries of the induced covariance matrix.
    Consumes O(m * n^2 + n^3) compute and O(m * n + n^2) memory instead of O(m^2 * n + m * n^2 + n^3) compute and O(m^2+n^2) memory necessary to compute the complete covariance matrix

    Args:
        mean (float): (m,) the constant prior mean of the to-be-conditioned Gaussian
        kernel (Callable): the prior kernel of the to-be-conditioned Gaussian
        kernel_params (dict): the parameters of the prior kernel such as length_scale and amplitude
        positions (jax.Array): (|X|, d) the positions of the elements of the domain
        observation_indices (jax.Array): (n,) the indices of the Gaussian components/coordinates that are observed
        observations (jax.Array): (n,) the actual, observed values
        observation_noise_stds (jax.Array): the standard deviations of the i.i.d. noise associated with the observations

    Returns:
        Tuple[jax.Array, jax.Array]: (the new mean vector, diagonal entries of the new covariance matrix in vector format)
    """
    mu_A = jnp.full((positions.shape[0],), mean) # (|X|)
    mu_B = jnp.full((observation_indices.size,), mean) # (n)

    diag_o_Sigma_AA = kernels._evaluate_kernel_nv_nv_diag(positions, positions, kernel, kernel_params) # O(|X|)
    Sigma_AB = kernels._evaluate_kernel_nv_nv(positions, positions[observation_indices, :], kernel, kernel_params) # O(|X| * n)
    Sigma_BA = jnp.matrix_transpose(Sigma_AB)
    Sigma_BB = Sigma_AB[observation_indices, :] + jnp.diag(observation_noise_stds**2)
    inv_Sigma_BB = jnp.linalg.inv(Sigma_BB)

    new_mean_vector = mu_A + Sigma_AB @ (inv_Sigma_BB @ (observations - mu_B))
    diag_o_new_covariance_matrix = diag_o_Sigma_AA - jnp.sum(Sigma_AB.transpose() * (inv_Sigma_BB @ Sigma_BA), axis=0)

    return (new_mean_vector, diag_o_new_covariance_matrix)

@jax.jit
def spsd_matrix_square_root(matrix:jax.Array) -> jax.Array:
    """Stable matrix square root based on the eigendecomposition of a symmetric positive semi-definite matrix. 
    The matrix is symmetrised if it is not symmetric.

    Args:
        matrix (jax.Array): (n, n) symmetric positive semi-definite matrix

    Returns:
        jax.Array : (n, n) matrix square root, i.e. for matrix = U D U^T it returns U sqrt(D),
    """
    d, U = jnp.linalg.eigh(matrix, symmetrize_input=True)
    d = jnp.maximum(d, 0)
    return U @ jnp.diag(d**.5)#, jnp.min(d), jnp.max(d))

@jax.jit
def sample(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, random_key:jax.Array) -> jax.Array:
    """Produces an m-dimensional random vector ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root (jax.Array): (m, m)
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (m,) a sample from N(mean_vector, covariance_matrix_square_root^2)
    """
    assert len(jnp.shape(mean_vector)) == 1, "the mean_vector must be a 1d array"
    assert jnp.shape(covariance_matrix_square_root) == (jnp.shape(mean_vector)[0], jnp.shape(mean_vector)[0]), "the covariance matrix must be of shape (m, m)"
    return covariance_matrix_square_root @ jax.random.normal(random_key, jnp.shape(mean_vector)) + mean_vector

_sample_n = jax.jit(jax.vmap(sample, in_axes=(None, None, 0), out_axes=0))

@partial(jax.jit, static_argnames=['n'])
def sample_n(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, n:int, random_key:jax.Array) -> jax.Array:
    """Produces n independent m-dimensional random vectors ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root (jax.Array): (m, m)
        n (int): number of samples
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (n, m) independent samples from N(mean_vector, covariance_matrix_square_root^2)
    """
    return _sample_n(mean_vector, covariance_matrix_square_root, jax.random.split(random_key, n))

@jax.jit
def sample_max(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, random_key:jax.Array) -> jax.Array:
    """Yields the maximum entry of an m-dimensional random vector ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root (jax.Array): (m, m)
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (1,) the maximum entry of a sample from N(mean_vector, covariance_matrix_square_root^2)
    """
    return jnp.max(sample(mean_vector, covariance_matrix_square_root, random_key))

@partial(jax.jit, static_argnames=['n', 'unroll'])
def sample_n_max(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, n:int, random_key:jax.Array, unroll:int=1) -> jax.Array:
    """Returns n independent samples from the maximum of m-dimensional random vectors ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root (jax.Array): (m, m)
        n (int): number of samples
        random_key (jax.Array): a single random key that seeds the sampling
        unroll (int): the number of times the loop is unrolled (more parallel computation & more memory consumption)

    Returns:
        jax.Array: (n,) independent samples from the maximum of N(mean_vector, covariance_matrix_square_root^2)
    """
    keys = jax.random.split(random_key, n)
    r = jnp.zeros((n,), dtype=float)
    @jax.jit
    def body_func(i:int, l:jax.Array):
        return l.at[i].set(sample_max(mean_vector, covariance_matrix_square_root, keys[i]))
    r = jax.lax.fori_loop(0, n, body_func, r, unroll=min(n, unroll)) # can be hardcoded to unroll to some extent
    return r

@jax.jit
def sample_arg_max(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, random_key:jax.Array) -> jax.Array:
    """Yields the index of the maximum entry of an m-dimensional random vector ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root (jax.Array): (m, m)
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (1,) the index of the maximum entry of a sample from N(mean_vector, covariance_matrix_square_root^2)
    """
    return jnp.argmax(sample(mean_vector, covariance_matrix_square_root, random_key))

@partial(jax.jit, static_argnames=['n', 'unroll'])
def sample_n_arg_max(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, n:int, random_key:jax.Array, unroll:int=1) -> jax.Array:
    """Returns n independent samples from the index of the maximum of m-dimensional random vectors ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root (jax.Array): (m, m)
        n (int): number of samples
        random_key (jax.Array): a single random key that seeds the sampling
        unroll (int): the number of times the loop is unrolled (more parallel computation & more memory consumption)

    Returns:
        jax.Array: (n,) independent samples from the index of the maximum of N(mean_vector, covariance_matrix_square_root^2)
    """
    keys = jax.random.split(random_key, n)
    r = jnp.zeros((n,), dtype=int)
    @jax.jit
    def body_func(i:int, l:jax.Array):
        return l.at[i].set(sample_arg_max(mean_vector, covariance_matrix_square_root, keys[i]))
    r = jax.lax.fori_loop(0, n, body_func, r, unroll=min(n, unroll))
    return r

@partial(jax.jit, static_argnames=['include_second_place'])
def sample_arg_max_and_max(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, random_key:jax.Array, include_second_place:bool) -> jax.Array:
    """Yields the (index, value) of the maximum entry of an m-dimensional random vector ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root: (jax.Array): (m, m)
        random_key (jax.Array): a single random key that seeds the sampling
        include_second_place (bool): whether to also return (index, value) for the second to maximum entry

    Returns:
        jax.Array: (2) the index, value of the maximum entry of a sample from N(mean_vector, covariance_matrix_square_root^2)
                or (2,2) the index, value of the maximum and second to maximum entry. Zeroth dimension selects between maximum and second to maximum, first dimension between index and value
    """
    f = sample(mean_vector, covariance_matrix_square_root, random_key) # O(?)
    x_star = jnp.argmax(f) # O(m)
    f_star = f[x_star]
    r = jnp.array((x_star, f_star))
    if include_second_place:
        x_starstar = jnp.argmax(f.at[x_star].set(-jnp.inf)) # exclude index x_star, O(m)
        f_starstar = f[x_starstar]
        r2 = jnp.array((x_starstar, f_starstar))
        r = jnp.stack((r, r2), axis=0)
    return r

def sample_n_arg_max_and_max(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, random_key:jax.Array, n:int, include_second_place:bool, unroll=1) -> jax.Array:
    """Returns n independent samples of (index, value) of the maximum entry of an m-dimensional random vector ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root (jax.Array): (m, m)
        random_key (jax.Array): a single random key that seeds the sampling
        n (int): number of samples
        include_second_place (bool): whether to also return (index, value) for the second-to-largest entry
        unroll (int): the number of times the loop is unrolled (more parallel computation & more memory consumption)

    Returns:
        jax.Array: (n, 2) n times the (index, value) of the largest entry of a sample from N(mean_vector, covariance_matrix_square_root^2)
                or (n, 2, 2) n times the (index, value) of the largest and second-to-largest entry. First dimension selects between maximum and second to maximum, second dimension between index and value
    """
    keys = jax.random.split(random_key, n)
    if include_second_place:
        r = jnp.zeros((n, 2, 2), dtype=float)
    else:
        r = jnp.zeros((n, 2), dtype=float)
    @jax.jit
    def body_func(i:int, l:jax.Array):
        return l.at[i].set(sample_arg_max_and_max(mean_vector, covariance_matrix_square_root, keys[i], include_second_place=include_second_place))
    r = jax.lax.fori_loop(0, n, body_func, r, unroll=min(n, unroll))
    return r

@jax.jit
def sample_max_w_products(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, random_key:jax.Array) -> jax.Array:
    """Yields (f^*, f^* * f) of a sample from the m-dimensional random vector f ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root: (jax.Array): (m, m)
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (1+m) with values (f^*, f^* * f) for f ~ N(mean_vector, covariance_matrix_square_root^2)
    """
    f = sample(mean_vector, covariance_matrix_square_root, random_key) # O(?), depends on implementation of TS
    x_star = jnp.argmax(f) * jnp.ones((1,), dtype=int) # O(m)
    f_star = f[x_star] * jnp.ones((1,)) 
    x_starstar = jnp.argmax(f.at[x_star].set(-jnp.inf)) # O(m)
    f_starstar = f[x_starstar] * jnp.ones((1,))
    return jnp.concatenate((x_star, f_star, f_star * f, f_starstar, f_starstar * f))

def sample_n_max_w_products(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, random_key:jax.Array, n:int, unroll=1) -> jax.Array:
    """Yields n independent samples of (f^*, f^* * f) where f ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root: (jax.Array): (m, m)
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (n, 1+m) with values (f^*, f^* * f) for f ~ N(mean_vector, covariance_matrix_square_root^2)
    """
    keys = jax.random.split(random_key, n)
    r = jnp.zeros((n, 3+2*mean_vector.size), dtype=float) 
    @jax.jit
    def body_func(i:int, l:jax.Array):
        return l.at[i, :].set(sample_max_w_products(mean_vector, covariance_matrix_square_root, keys[i]))
    r = jax.lax.fori_loop(0, n, body_func, r, unroll=min(n, unroll))
    return r

@partial(jax.jit, static_argnames=['n', 'unroll'])
def count_sample_n_arg_max(mean_vector:jax.Array, covariance_matrix_square_root:jax.Array, n:int, random_key:jax.Array, unroll:int=1) -> jax.Array:
    """Counts the occurances of indices of n independent samples of the maximum of m-dimensional random vectors ~ N(mean_vector, covariance_matrix_square_root^2)

    Args:
        mean_vector (jax.Array): (m,)
        covariance_matrix_square_root (jax.Array): (m, m)
        n (int): number of samples
        random_key (jax.Array): a single random key that seeds the sampling
        unroll (int): the number of times the loop is unrolled (more parallel computation & more memory consumption)

    Returns:
        jax.Array: (n,) counts of indices of independent samples from the maximum of N(mean_vector, covariance_matrix_square_root^2)
    """
    counts = jnp.zeros(jnp.shape(mean_vector), dtype=int)
    @jax.jit
    def body_func(i:int, state:Tuple[jax.Array, jax.Array]):
        counts = state[0]
        key = state[1]
        new_key, one_time_key = jax.random.split(key)
        return (counts.at[sample_arg_max(mean_vector, covariance_matrix_square_root, one_time_key)].add(1), new_key)
    r = jax.lax.fori_loop(0, n, body_func, (counts, random_key), unroll=min(n,unroll))
    return r[0]

@jax.jit
def sample_independent(mean_vector:jax.Array, standard_deviations:jax.Array, random_key:jax.Array) -> jax.Array:
    """Produces an m-dimensional random vector ~ N(mean_vector, diag(standard_deviations^2))

    Args:
        mean_vector (jax.Array): (m,)
        standard_deviations (jax.Array): (m,)
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (m,) a sample from N(mean_vector, diag(standard_deviations^2))
    """
    assert len(jnp.shape(mean_vector)) == 1, "the mean_vector must be a 1d array"
    assert jnp.shape(standard_deviations) == jnp.shape(mean_vector), "the mean vector and standard deviations must have equal shape"
    return standard_deviations * jax.random.normal(random_key, jnp.shape(mean_vector)) + mean_vector

_sample_n_independent = jax.jit(jax.vmap(sample_independent, in_axes=(None, None, 0), out_axes=0))

@partial(jax.jit, static_argnames=['n'])
def sample_n_independent(mean_vector:jax.Array, standard_deviations:jax.Array, n:int, random_key:jax.Array) -> jax.Array:
    """Produces n independent m-dimensional random vectors ~ N(mean_vector, diag(standard_deviations^2))

    Args:
        mean_vector (jax.Array): (m,)
        standard_deviations (jax.Array): (m,)
        n (int): number of samples
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (n, m) independent samples from N(mean_vector, diag(standard_deviations^2))
    """
    return _sample_n_independent(mean_vector, standard_deviations, jax.random.split(random_key, n))

@jax.jit
def sample_max_independent(mean_vector:jax.Array, standard_deviations:jax.Array, random_key:jax.Array) -> jax.Array:
    """Yields the maximum entry of an m-dimensional random vector ~ N(mean_vector, diag(standard_deviations^2))

    Args:
        mean_vector (jax.Array): (m,)
        standard_deviations (jax.Array): (m,)
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (1,) the maximum entry of a sample from N(mean_vector, diag(standard_deviations^2))
    """
    return jnp.max(sample_independent(mean_vector, standard_deviations, random_key))

@partial(jax.jit, static_argnames=['n', 'unroll'])
def sample_n_max_independent(mean_vector:jax.Array, standard_deviations:jax.Array, n:int, random_key:jax.Array, unroll:int=1) -> jax.Array:
    """Returns n independent samples from the maximum of m-dimensional random vectors ~ N(mean_vector, diag(standard_deviations^2))

    Args:
        mean_vector (jax.Array): (m,)
        standard_deviations (jax.Array): (m,)
        n (int): number of samples
        random_key (jax.Array): a single random key that seeds the sampling
        unroll (int): the number of times the loop is unrolled (more parallel computation & more memory consumption)

    Returns:
        jax.Array: (n,) independent samples from the maximum of N(mean_vector, diag(standard_deviations^2))
    """
    keys = jax.random.split(random_key, n)
    r = jnp.zeros((n,), dtype=float)
    @jax.jit
    def body_func(i:int, l:jax.Array):
        return l.at[i].set(sample_max_independent(mean_vector, standard_deviations, keys[i]))
    r = jax.lax.fori_loop(0, n, body_func, r, unroll=min(n, unroll)) # can be hardcoded to unroll to some extent
    return r

@jax.jit
def sample_arg_max_independent(mean_vector:jax.Array, standard_deviations:jax.Array, random_key:jax.Array) -> jax.Array:
    """Yields the index of the maximum entry of an m-dimensional random vector ~ N(mean_vector, diag(standard_deviations^2))

    Args:
        mean_vector (jax.Array): (m,)
        standard_deviations (jax.Array): (m,)
        random_key (jax.Array): a single random key that seeds the sampling

    Returns:
        jax.Array: (1,) the index of the maximum entry of a sample from N(mean_vector, diag(standard_deviations^2))
    """
    return jnp.argmax(sample_independent(mean_vector, standard_deviations, random_key))

@partial(jax.jit, static_argnames=['n', 'unroll'])
def sample_n_arg_max_independent(mean_vector:jax.Array, standard_deviations:jax.Array, n:int, random_key:jax.Array, unroll:int=1) -> jax.Array:
    """Returns n independent samples from the index of the maximum of m-dimensional random vectors ~ N(mean_vector, diag(standard_deviations^2))

    Args:
        mean_vector (jax.Array): (m,)
        standard_deviations (jax.Array): (m,)
        n (int): number of samples
        random_key (jax.Array): a single random key that seeds the sampling
        unroll (int): the number of times the loop is unrolled (more parallel computation & more memory consumption)

    Returns:
        jax.Array: (n,) independent samples from the index of the maximum of N(mean_vector, diag(standard_deviations^2))
    """
    keys = jax.random.split(random_key, n)
    r = jnp.zeros((n,), dtype=int)
    @jax.jit
    def body_func(i:int, l:jax.Array):
        return l.at[i].set(sample_arg_max_independent(mean_vector, standard_deviations, keys[i]))
    r = jax.lax.fori_loop(0, n, body_func, r, unroll=min(n, unroll))
    return r

@partial(jax.jit, static_argnames=['n', 'unroll'])
def count_sample_n_arg_max_independent(mean_vector:jax.Array, standard_deviations:jax.Array, n:int, random_key:jax.Array, unroll:int=1) -> jax.Array:
    """Counts the occurances of indices of n independent samples of the maximum of m-dimensional random vectors ~ N(mean_vector, diag(standard_deviations^2))

    Args:
        mean_vector (jax.Array): (m,)
        standard_deviations (jax.Array): (m,)
        n (int): number of samples
        random_key (jax.Array): a single random key that seeds the sampling
        unroll (int): the number of times the loop is unrolled (more parallel computation & more memory consumption)

    Returns:
        jax.Array: (n,) counts of indices of independent samples from the maximum of N(mean_vector, diag(standard_deviations^2))
    """
    counts = jnp.zeros(jnp.shape(mean_vector), dtype=int)
    @jax.jit
    def body_func(i:int, state:Tuple[jax.Array, jax.Array]):
        counts = state[0]
        key = state[1]
        new_key, one_time_key = jax.random.split(key)
        return (counts.at[sample_arg_max_independent(mean_vector, standard_deviations, one_time_key)].add(1), new_key)
    r = jax.lax.fori_loop(0, n, body_func, (counts, random_key), unroll=min(n,unroll))
    return r[0]

@partial(jax.jit, static_argnames=['include_second_place'])
def sample_arg_max_and_max_independent(mean_vector:jax.Array, standard_deviations:jax.Array, random_key:jax.Array, include_second_place:bool) -> jax.Array:
    """Yields the (index, value) of the maximum entry of an m-dimensional random vector ~ N(mean_vector, diag(standard_deviations^2))

    Args:
        mean_vector (jax.Array): (m,)
        standard_deviations (jax.Array): (m,)
        random_key (jax.Array): a single random key that seeds the sampling
        include_second_place (bool): whether to also return (index, value) for the second to maximum entry

    Returns:
        jax.Array: (2) the index, value of the maximum entry of a sample from N(mean_vector, diag(standard_deviations^2))
                or (2,2) the index, value of the maximum and second to maximum entry. Zeroth dimension selects between maximum and second to maximum, first dimension between index and value
    """
    f = sample_independent(mean_vector, standard_deviations, random_key) # O(m)
    x_star = jnp.argmax(f) # O(m)
    f_star = f[x_star]
    r = jnp.array((x_star, f_star))
    if include_second_place:
        x_starstar = jnp.argmax(f.at[x_star].set(-jnp.inf)) # exclude index x_star, O(m)
        f_starstar = f[x_starstar]
        r2 = jnp.array((x_starstar, f_starstar))
        r = jnp.stack((r, r2), axis=0)
    return r

@partial(jax.jit, static_argnames=['include_second_place', 'n', 'unroll'])
def sample_n_arg_max_and_max_independent(mean_vector:jax.Array, standard_deviations:jax.Array, random_key:jax.Array, n:int, include_second_place:bool, unroll=1) -> jax.Array:
    """Returns n independent samples of (index, value) of the maximum entry of an m-dimensional random vector ~ N(mean_vector, diag(standard_deviations^2))

    Args:
        mean_vector (jax.Array): (m,)
        standard_deviations (jax.Array): (m,)
        random_key (jax.Array): a single random key that seeds the sampling
        n (int): number of samples
        include_second_place (bool): whether to also return (index, value) for the second-to-largest entry
        unroll (int): the number of times the loop is unrolled (more parallel computation & more memory consumption)

    Returns:
        jax.Array: (n, 2) n times the (index, value) of the largest entry of a sample from N(mean_vector, diag(standard_deviations^2))
                or (n, 2, 2) n times the (index, value) of the largest and second-to-largest entry. First dimension selects between maximum and second to maximum, second dimension between index and value
    """
    keys = jax.random.split(random_key, n)
    if include_second_place:
        r = jnp.zeros((n, 2, 2), dtype=float)
    else:
        r = jnp.zeros((n, 2), dtype=float)
    @jax.jit
    def body_func(i:int, l:jax.Array):
        return l.at[i].set(sample_arg_max_and_max_independent(mean_vector, standard_deviations, keys[i], include_second_place=include_second_place))
    r = jax.lax.fori_loop(0, n, body_func, r, unroll=min(n, unroll))
    return r

@jax.jit
def variances(covariance_matrix:jax.Array) -> jax.Array:
    """Returns the variances, i.e. the diagonal

    Args:
        covariance_matrix (jax.Array): (n, n) symmetric positive semi-definite matrix

    Returns:
        jax.Array: (n,) variances
    """
    return jnp.diag(covariance_matrix)

@jax.jit
def standard_deviations(covariance_matrix:jax.Array) -> jax.Array:
    """Returns the standard deviations, i.e. the square root of the diagonal

    Args:
        covariance_matrix (jax.Array): (n, n) symmetric positive semi-definite matrix

    Returns:
        jax.Array: (n,) standard deviations
    """
    return variances(covariance_matrix)**.5

@jax.jit
def entropy(covariance_matrix:jax.Array) -> jax.Array:
    """Returns the entropy of a multivariate Gaussian

    Args:
        covariance_matrix (jax.Array): (n, n) symmetric positive semi-definite matrix

    Returns:
        jax.Array: entropy
    """
    (sign, logabsdet) = jnp.linalg.slogdet(covariance_matrix) # numerically stable log determinant
    return 0.5*(logabsdet + jnp.shape(covariance_matrix)[0]*(1+jnp.log(2*jnp.pi)))